{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "！记得补充对应英文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AI > Machine Learning > Deep Learning\n",
    "\n",
    "Machine learning algorithm:\n",
    "    1. Decision Tree\n",
    "    2. Random Forest\n",
    "    3. XGBoost\n",
    "    4. Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Decision Tree 单一树状结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Main idea: 基于features递归分割数据，分裂标准有信息增益和基尼系数（？具体区别）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Random Forest 多棵决策树，集成模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Main idea: 对原始数据有放回地抽样（bootstrap），产生与原始数据集大小相同的子集，每个样本被抽到的概率是 p=1-((n-1)/n)^n=63.2%\n",
    "           当n足够大时（>=1000），意味着每个树的样本中都有63.2%的独特样本，每两棵树之间有39.9%的重复样本，只在一棵树的样本为46.5%，两棵树都没有的样本为13.5%\n",
    "           当n比较小时，如n=50，这一比例会变小\n",
    "           以这种抽样方法构造出很多子集，以此训练对应数量的决策树，这些决策树之间是弱相关的（还是没懂为什么弱相关）\n",
    "           最后根据这么多棵决策树的共同结果给出决策，分类用众数mode，回归取平均\n",
    "           补充：（1）存在 out of bag samples袋外样本，可以用于验证\n",
    "                 （2）确定单棵树的样本后会进行特征抽样feature bagging，随机森林模型中包含max_features参数，该参数常见取值有auto/sqrt/log2等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. XGBoost 集成模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Main idea: 按顺序训练多棵树，每个树学习前一棵树的残差，通过步步逼近防止过拟合\n",
    "           优化目标：损失函数 + 正则项（防过拟合）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Support Vector Machine 几何分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Main idea: 寻找最大化分类间隔的超平面\n",
    "           核技巧：通过核函数处理非线性分割，核函数也有好几种"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code\n",
    "#### 1. 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# picture\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 绘图代码执行后立刻显示在代码框下\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 只返回分数\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# 还返回如响应时间等其他数据，字典格式\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# 不同调参方式\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "# 决策树需要\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn import tree\n",
    "\n",
    "# 随机森林需要\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# XGBoost需要\n",
    "import xgboost as xgb\n",
    "\n",
    "# SVM需要\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)\n",
    "warnings.filterwarnings('ignore',category=DeprecationWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset\n",
    "\n",
    "# 逗号分隔：sep=','，pd.read_table可以自定义分隔符\n",
    "\n",
    "fires = pd.read_table(\"C:/Users/ruhan/Desktop/Classes/2025 ML and NLP/Week 2/Algerian_forest_fires_dataset.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine data\n",
    "\n",
    "type(fires)\n",
    "fires.shape\n",
    "fires.columns\n",
    "fires.head()\n",
    "fires.info()\n",
    "fires.describe()\n",
    "fires.isnull().sum()\n",
    "\n",
    "# Examine categorical variables \n",
    "cat_vars = ['PD',\"Classes\"]\n",
    "for col in cat_vars:\n",
    "    print(fires[col].value_counts())\n",
    "    \n",
    "# Modify data\n",
    "\n",
    "fires = fires.drop(['day','month','year'],axis=1)\n",
    "fires[\"Classes\"].unique()\n",
    "fires[\"Classes\"] = fires[\"Classes\"].str.strip()\n",
    "fires= pd.get_dummies(fires, columns = ['Classes'])\n",
    "fires[\"Classes\"].value_counts()\n",
    "sns.countplot(fires[\"Classes\"], label = \"Count\")\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x = 'Temperature', y = 'FFMC', hue = 'Classes', data = fires)\n",
    "\n",
    "# Let's check the correlation between the variables \n",
    "plt.figure(figsize=(20,10)) \n",
    "sns.heatmap(fires.corr(), annot=True) \n",
    "\n",
    "# Drop BUI variable with less interpretability and high correlation \n",
    "fires = fires.drop(['BUI'],axis=1)\n",
    "\n",
    "# Handle categorial variables \n",
    "ord_enco = OrdinalEncoder(categories=[['Low ', 'Medium', 'High']])\n",
    "en_PD = ord_enco.fit_transform(fires.loc[:, ['PD']]) #\n",
    "en_PD = pd.DataFrame(en_PD)\n",
    "en_PD  = en_PD.rename(columns= {0:'PDnew'})\n",
    "fires['PDnew'] = ord_enco.fit_transform(fires.loc[:, ['PD']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into features and target\n",
    "\n",
    "target = fires['Classes']\n",
    "features = fires.drop('Classes', axis=1)\n",
    "\n",
    "# split into test and train set\n",
    "X_train,X_test,y_train,y_test = train_test_split(features,target,test_size=0.2,random_state=42)\n",
    "X_train.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 初始化各分类器 Initialize the classifier with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree\n",
    "# if we only use default, no hyperparameter tuning or cross validation\n",
    "Classifier_DT = DecisionTreeClassifier(random_state=42)\n",
    "Classifier_DT.get_params()\n",
    "\n",
    "# Random forest\n",
    "Classifier_RF = RandomForestClassifier(random_state=42)\n",
    "Classifier_RF = RandomForestClassifier(n_estimators=2,random_state=1)\n",
    "Classifier_RF.get_params()\n",
    "RandomForestClassifier().get_params()\n",
    "\n",
    "# XGBoost\n",
    "Classifier_Xgb = xgb.XGBClassifier(random_state=42)\n",
    "Classifier_Xgb = xgb.XGBClassifier(n_estimators=100, objective='binary:logistic', eta=0.1, max_depth=3)\n",
    "# method 1 查看定义好的包含部分参数的\n",
    "Classifier_Xgb.get_params()\n",
    "# method 2 查看最原始的没有任何参数的格式\n",
    "xgb.XGBClassifier().get_params()\n",
    "\n",
    "# Support Vector Machine\n",
    "Classifier_SVC= SVC()\n",
    "Classifier_SVC.get_params()\n",
    "Classifier_SVC.get_params()\n",
    "SVC().get_params()\n",
    "\n",
    "# Train the classifier\n",
    "Classifier_DT.fit(X_train, y_train)\n",
    "Classifier_RF.fit(X_train, y_train)\n",
    "Classifier_Xgb.fit(X_train, y_train)\n",
    "# SVC训练前特征必须标准化\n",
    "Classifier_SVC.fit(X_train, y_train)\n",
    "\n",
    "# calculate cross validation score \n",
    "cv_results_DT = cross_val_score(Classifier_DT, X_train, y_train, cv=5)\n",
    "cv_results_RF = cross_val_score(Classifier_RF, X_train, y_train, cv=5)\n",
    "cv_results_Xgb = cross_val_score(Classifier_Xgb, X_train, y_train, cv=5)\n",
    "cv_results_SVC = cross_val_score(Classifier_SVC, X_train, y_train, cv=5)\n",
    "\n",
    "\n",
    "# Evaluate 评估\n",
    "# Decision tree\n",
    "print(classification_report(y_test, Classifier_DT.predict(X_test)))\n",
    "# Create confusion matrix\n",
    "print(confusion_matrix(y_test,Classifier_DT.predict(X_test)))\n",
    "\n",
    "# Visualize the tree\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12, 6))\n",
    "tree.plot_tree(clf, feature_names=X_train.columns, class_names=clf.classes_, filled=True)\n",
    "plt.show()\n",
    "\n",
    "#  Random forest\n",
    "y_predict = Classifier_RF.predict(X_test)\n",
    "print('Accuracy: %.3f' % accuracy_score(np.array(y_test), y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1  Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.1 Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum depth of the tree\n",
    "max_depth = [None, 5, 10, 20]\n",
    "\n",
    "# Minimum number of samples required to split an internal node\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# Minimum number of samples required at a leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# Criterion for measuring quality of split\n",
    "criterion = ['gini', 'entropy']\n",
    "\n",
    "# Create a random grid\n",
    "rnd_grid = {\n",
    "    'max_depth': max_depth,\n",
    "    'min_samples_split': min_samples_split,\n",
    "    'min_samples_leaf': min_samples_leaf,\n",
    "    'criterion': criterion\n",
    "}\n",
    "\n",
    "print(rnd_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.2 Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of trees \n",
    "n_estimator = [int(x) for x in np.linspace(start =200, stop=1000,num=10)]\n",
    "# number of feature at each split\n",
    "max_features = ['sqrt']\n",
    "# max depth \n",
    "max_depth = [int(x) for x in np.linspace(start =10, stop=100,num=10)]\n",
    "# min number of sample at each split \n",
    "min_sam_split = [2,5,10]\n",
    "\n",
    "\n",
    "# Create a random grid \n",
    "rnd_grid= {'n_estimators':n_estimator,\n",
    "          'max_features':max_features,\n",
    "          'max_depth' :max_depth,\n",
    "          'min_samples_split' :min_sam_split}\n",
    "\n",
    "print(rnd_grid)\n",
    "\n",
    "# Use randomizedsearchcv\n",
    "Classifier_RF_random = RandomizedSearchCV(estimator=Classifier_RF,param_distributions =rnd_grid,\n",
    "                               n_iter = 10,cv=3,verbose=2,random_state=33,n_jobs=-1,return_train_score=True)\n",
    "Classifier_RF_random.fit(X_train, y_train)\n",
    "\n",
    "# Examine the results from each combination of hyperparameters \n",
    "pd.DataFrame(Classifier_RF_random.cv_results_)\n",
    "\n",
    "print('Best hyper parameter:', Classifier_RF_random.best_params_)\n",
    "\n",
    "# Evaluate\n",
    "best_random = Classifier_RF_random.best_estimator_\n",
    "best_random.fit(X_train,y_train)\n",
    "y_pred_rnd = best_random.predict(X_test)\n",
    "rnd_accuracy = accuracy_score(np.array(y_test), y_pred_rnd)\n",
    "\n",
    "print('Random best hyperparameter accuracy: ',rnd_accuracy)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_rnd)\n",
    "cm\n",
    "\n",
    "# View fature scores \n",
    "feature_score = pd.Series(best_random.feature_importances_,index=X_train.columns).sort_values(ascending =False)\n",
    "feature_score\n",
    "\n",
    "# Visualize feature importance \n",
    "sns.barplot(x=feature_score,y=feature_score.index)\n",
    "plt.xlabel(\"Feature importance\")\n",
    "plt.ylabel(\"Feature name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.3 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max depth \n",
    "max_depth = [3,6,10]\n",
    "# Learning rate  \n",
    "learning_rate = [0.01,0.1]\n",
    "# number of trees \n",
    "n_estimator = [50,100]\n",
    "# col sample by tree \n",
    "col_sample = [0.6,1]\n",
    "\n",
    "\n",
    "# Create a random grid \n",
    "rnd_grid= {'n_estimators':n_estimator,\n",
    "          'learning_rate':learning_rate,\n",
    "          'max_depth' :max_depth,\n",
    "          'colsample_bytree' :col_sample}\n",
    "\n",
    "print(rnd_grid)\n",
    "\n",
    "# Use randomizedsearchcv\n",
    "Classifier_Xgb_random = RandomizedSearchCV(estimator=Classifier_Xgb,param_distributions =rnd_grid,\n",
    "                               n_iter = 10,cv=3,verbose=2,random_state=33,n_jobs=-1,return_train_score=True)\n",
    "Classifier_Xgb_random.fit(X_train, y_train)\n",
    "\n",
    "# Examine the results from each combination of hyperparameters \n",
    "pd.DataFrame(Classifier_Xgb_random.cv_results_)\n",
    "\n",
    "print('Best hyper parameter:', Classifier_Xgb_random.best_params_)\n",
    "\n",
    "# Evaluate \n",
    "best_random = Classifier_Xgb_random.best_estimator_\n",
    "best_random.fit(X_train, y_train)\n",
    "y_pred_best = best_random.predict(X_test)\n",
    "rnd_accuracy = accuracy_score(np.array(y_test), y_pred_best)\n",
    "\n",
    "print('Random best hyperparameter accuracy: ',rnd_accuracy)\n",
    "\n",
    "# Display the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "cm\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_random.classes_)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Confusion Matrix - Test Set\")\n",
    "plt.show()\n",
    "\n",
    "# View fature scores \n",
    "feature_score = pd.Series( best_random.feature_importances_,index=X_train.columns).sort_values(ascending =False)\n",
    "feature_score\n",
    "\n",
    "# Visualize feature importance \n",
    "sns.barplot(x=feature_score,y=feature_score.index)\n",
    "plt.xlabel(\"Feature importance\")\n",
    "plt.ylabel(\"Feature name\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.4 SVM\n",
    "先特征标准化再训练调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标准化\n",
    "scaler = StandardScaler()\n",
    "cols = X_train.columns\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = pd.DataFrame(X_train, columns=[cols])\n",
    "X_test = pd.DataFrame(X_test, columns=[cols])\n",
    "\n",
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel type\n",
    "kernel = ['linear','rbf']\n",
    "\n",
    "# C parameter \n",
    "C = [0.1,1,10]\n",
    "\n",
    "\n",
    "# Create a grid \n",
    "h_params= {'kernel':kernel,\n",
    "          'C':C}\n",
    "\n",
    "print(h_params)\n",
    "\n",
    "# Use grid search\n",
    "Classifier_SVC_grid=GridSearchCV(Classifier_SVC,h_params,cv=5)\n",
    "Classifier_SVC_grid.fit(X_train, y_train)\n",
    "\n",
    "# Examine the results from each combination of hyperparameters \n",
    "pd.DataFrame(Classifier_SVC_grid.cv_results_)\n",
    "print('Best hyper parameter:', Classifier_SVC_grid.best_params_)\n",
    "\n",
    "# Evaluate \n",
    "best_grid = Classifier_SVC_grid.best_estimator_\n",
    "best_grid.fit(X_train, y_train)\n",
    "y_pred_best = best_grid.predict(X_test)\n",
    "rnd_accuracy = accuracy_score(np.array(y_test), y_pred_best)\n",
    "\n",
    "print('Grid best hyperparameter accuracy: ',rnd_accuracy)\n",
    "\n",
    "# Display the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "cm\n",
    "\n",
    "plot_confusion_matrix(svc_best, X_test, y_test)  \n",
    "plt.show()\n",
    "\n",
    "# View fature scores \n",
    "# linear\n",
    "pd.Series(abs(svc_best.coef_[0]),index=X_train.columns).sort_values(ascending =True).plot(kind='barh')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
